# -*- coding: utf-8 -*-
"""Modelo_de_Partidos_PoliticosNB.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1XpfG27j-a5j9sH6P6Vw4yXU_NCCSZ9G3

ANALIS DE DATOS
"""

#instalamos las librerias
!pip install clean-text[gpl]
!pip install py4j
!pip install -q findspark
!pip install tqdm 
!pip install plotly 
!pip install pyspark==2.4.0

#@title Texto de tÃ­tulo predeterminado
!apt-get install openjdk-8-jdk-headless -qq > /dev/null
!wget -q https://www.dropbox.com/s/96rr4avobbz9a1p/spark-2.4.0-bin-hadoop2.7.tar
!tar xf spark-2.4.0-bin-hadoop2.7.tar

!wget https://www.dropbox.com/s/np2v706jyz4yz44/data_extended.csv

from google.colab import drive
drive.mount('/content/gdrive')

# Commented out IPython magic to ensure Python compatibility.
#importamos la librerias necesarias para el preprocesamiento
# %matplotlib inline
from nltk import TweetTokenizer
from sklearn.metrics import roc_curve,auc
import pandas as pd
import numpy as np
import nltk
from nltk.corpus import stopwords
from nltk.stem import SnowballStemmer
import re
from string import punctuation
from cleantext import clean
import os
import matplotlib.pyplot as plt
from gensim.models import KeyedVectors
from sklearn.datasets import fetch_20newsgroups

from pyspark.sql import SparkSession
from pyspark import SparkContext
from pyspark.sql.types import *
from pyspark.sql import *
from pyspark.sql import Row
from pyspark.sql import functions as func
from pyspark.mllib.regression import LabeledPoint
from pyspark.mllib.regression import LinearRegressionModel
from pyspark.ml.classification import RandomForestClassifier
from pyspark.ml.feature import  SQLTransformer
from pyspark.ml.evaluation import  BinaryClassificationEvaluator
from pyspark.ml.classification import NaiveBayes
from pyspark.mllib.evaluation import MulticlassMetrics
from pyspark.ml.evaluation import MulticlassClassificationEvaluator
from pyspark.ml.feature import (VectorAssembler,StringIndexer,StandardScaler,
                                OneHotEncoderEstimator,OneHotEncoder, 
                                VectorIndexer, IndexToString,
                                HashingTF, Tokenizer)
from pyspark.ml.tuning import ParamGridBuilder, CrossValidator
from pyspark.ml import Pipeline, PipelineModel
from pyspark.sql import Row
from pyspark.ml.classification import LinearSVC,OneVsRest,LogisticRegression
from pyspark.ml.clustering import KMeans
from pyspark.ml.evaluation import ClusteringEvaluator

import os
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"
os.environ["SPARK_HOME"] = "/content/spark-2.4.0-bin-hadoop2.7"

import findspark
findspark.init("/content/spark-2.4.0-bin-hadoop2.7")

# funcion para la matriz de confusion
def plot_matrix_confusion(lr_cv_predictions,
                          normalize=False,
                          title='Confusion matrix',
                          cmap=plt.cm.Blues):
  from numpy import array
  import itertools
  
  class_temp = lr_cv_predictions.select("label").groupBy("label")\
                              .count().sort('count', ascending=False).toPandas()
  class_temp = class_temp["label"].tolist()
  class_names = list(map(str, class_temp))
  
    
  predictions_and_labels = lr_cv_predictions.select("prediction", "label").rdd.map(lambda r: (float(r[0]), float(r[1])))
  metrics = MulticlassMetrics(predictions_and_labels)
  confusion_matrix = metrics.confusionMatrix().toArray()
  met_list = array([[int(x[0]), int(x[1]),int(x[2])] for x in confusion_matrix])

    
  cm = met_list
  classes = class_names
  if normalize:
      cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]

  plt.imshow(cm, interpolation='nearest', cmap=cmap)
  plt.title(title)
  plt.colorbar()
  tick_marks = np.arange(len(classes))
  plt.xticks(tick_marks, classes, rotation=45)
  plt.yticks(tick_marks, classes)

  fmt = '.2f' if normalize else 'd'
  thresh = cm.max() / 2.
  for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
      plt.text(j, i, format(cm[i, j], fmt),
               horizontalalignment="center",
               color="white" if cm[i, j] > thresh else "black")

  plt.tight_layout()
  plt.ylabel('True label')
  plt.xlabel('Predicted label')
  plt.show()

def plot_roc_curve2(cv_prediction):
  #PREDICTIONS
  predictions_pddf = cv_prediction.select('prediction','label').toPandas()
  prob = predictions_pddf["prediction"] 
  fpr, tpr, thresholds = roc_curve(predictions_pddf['label'], prob, pos_label=1);
  roc_auc = auc(fpr, tpr)

  # PLOT ROC CURVES
  plt.figure(figsize=(5,5))
  plt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % roc_auc)
  plt.plot([0, 1], [0, 1], 'k--')
  plt.xlim([0.0, 1.0])
  plt.ylim([0.0, 1.05])
  plt.xlabel('False Positive Rate')
  plt.ylabel('True Positive Rate')
  plt.title('ROC Curve')
  plt.legend(loc="lower right")
  plt.show()

def get_multiclass_evaluator(prediction):
  lr_multi_evaluator = MulticlassClassificationEvaluator(labelCol='label', predictionCol='prediction')

  # Accuracy
  lr_accuracy = lr_multi_evaluator.evaluate(prediction, {lr_multi_evaluator.metricName: "accuracy"})
  print("Accuracy: {:.2f}".format(lr_accuracy))

  # f1
  f1 = lr_multi_evaluator.evaluate(prediction, {lr_multi_evaluator.metricName: "f1"})
  print("f1: {:.2f}".format(f1))

  # Precision
  wp = lr_multi_evaluator.evaluate(prediction, {lr_multi_evaluator.metricName: "weightedPrecision"})
  print("Precision: {:.2f}".format(wp))

  # Recall
  wr = lr_multi_evaluator.evaluate(prediction, {lr_multi_evaluator.metricName: "weightedRecall"})
  print("Recall: {:.2f}".format(wr))
  
  return lr_multi_evaluator

def get_tunning_hiperparameter(grid, cv_model):
  bestPipeline = cv_model.bestModel
  bestLRModel = bestPipeline.stages[-1]
  result = {param.name: bestLRModel.getOrDefault(bestLRModel.getParam(param.name)) for param in grid[0]}
  return result

spark = SparkSession.builder.master("local[*]").appName("Pricing").getOrCreate()

drive.mount("/content/drive")

dataset = pd.read_csv(path, sep=",")

#pip install openpyxl
#train_data.to_csv('data.csv')
#!cp data.csv "drive/My Drive/"

dataset


dataset

DIACRITICAL_VOWELS = [('Ã¡','a'), ('Ã©','e'), ('Ã­','i'), ('Ã³','o'), ('Ãº','u'), ('Ã¼','u')]
SLANG = [('d','de'), ('[qk]','que'), ('xo','pero'), ('xa', 'para'), ('[xp]q','porque'),('es[qk]', 'es que'),
              ('fvr','favor'),('(xfa|xf|pf|plis|pls|porfa)', 'por favor'), ('dnd','donde'), ('tb', 'tambiÃ©n'),
              ('(tq|tk)', 'te quiero'), ('(tqm|tkm)', 'te quiero mucho'), ('x','por'), ('\+','mas'),
              ('piÃ±a','mala suerte'),('agarre','adulterio'),('ampay','verguenza'),('bacan','alegria'),
              ('bamba','falsificado'),('cabeceador','ladron'),('cabro','homosexual'),('cachaciento','burlon'),
              ('calabacita','tonta'),('caleta','secreto'),('cabro','homosexual'),('cana','carcel'),
              ('chucha','molestia'),('choro','ladron'),('conchÃ¡n','conchudo'),('cutra','ilicito'),
              ('dark','horrible'),('lenteja','torpe'),('lorna','tonto'),('mancar','morir'),
              ('monse','tonto'),('lenteja','torpe'),('lorna','tonto'),('mancar','morir'),('piÃ±ata','mala suerte')
              ]

SIMBOLOS_POSITIVOS=[('ğŸ˜€','agradecimiento'), ('ğŸ˜','entusiasmo'), ('ğŸ˜ƒ','alegria'), ('ğŸ˜„','felicidad'), ('ğŸ˜…','optimismo'), ('ğŸ˜†','risa'),
                   ('ğŸ’ª','esfuerzo'),(':\)','alegria'),('ğŸ’š','amor'),('ğŸ’','amor'),('ğŸ˜‡','angel'),(':â€‘\)','sonrisa'),(':-]','sonrisa'),
                    (':3','sonrisa'),(':-3','sonrisa'),(':>','sonrisa'),(':->','sonrisa'),('8\)','sonrisa'),(':}','sonrisa'),
                   (':-}','sonrisa'),(':c','sonrisa'),(':o','sonrisa'),(':^\)','sonrisa'),('=]','sonrisa'),('=\)','sonrisa'),('B^D','risa'),
                    (':D','risa'),('8D','risa'),('xD','risa'),('8â€‘D','risa'),('xâ€‘D','risa'),(':â€‘D','risa'),
                    ('=D','risa'),('=3','risa'),(':-\)\)','feliz'),(':-*','beso'),(':*','beso'),(':Ã—','beso'),(';\)','GuiÃ±o'),
                     (';â€‘\)','GuiÃ±o'),(';\]','GuiÃ±o'),(';â€‘]','GuiÃ±o'),(':â€‘,','GuiÃ±o'),(';^\)','GuiÃ±o'),(';D','GuiÃ±o'),('O:â€‘\)','Angel'),
                    ('ğŸ˜','Gusto'),('ğŸ˜Œ','Relajo'),('ğŸ˜™','Felicidad'),('ğŸ˜œ','GuiÃ±o'),('ğŸ˜¸','Alegria'),('ğŸ˜¸','Alegria'),('ğŸ˜¹','Alegria'),('ğŸ˜»','Alegria')
                   ,('ğŸ™Œ','exito'),('ğŸ‘','admiracion'),('âœŒ','amor y paz'),('ğŸ‘Œ','aprobacion'),('â˜‘','aprobacion')]

SIMBOLOS_NEGATIVOS=[(':â€‘\(','triste'), (':\(','triste'), (':c','triste'), (':â€‘c','triste'), (':â€‘<','triste'), (':<','triste')
                    ,(':â€‘\[','triste'),(':\[','triste'),(':-||','triste'),(':{','triste'),(':{','triste'),('Dâ€‘\':','disgusto')
                  ,(':\\','dubitativo'),('=/','indeciso'),(':L','indeciso'),('=L','indeciso'),(':S','indeciso'),(':$','indeciso'),('v.v','indeciso')
                   ]

stop_words=['a', 'actualmente', 'adelante', 'ademÃ¡s', 'afirmÃ³', 'agregÃ³', 'ahÃ­', 'ahora',
    'cc', 'pa', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l',
    'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'al',
    'algo', 'algÃºn', 'algÃºn', 'alguna', 'algunas', 'alguno', 'algunos',
    'alrededor', 'ambos', 'ampleamos', 'aÃ±adiÃ³', 'ante', 'anterior', 'antes',
    'apenas', 'aproximadamente', 'aquel', 'aquellas', 'aquellos', 'aqui',
    'aquÃ­', 'arriba', 'asegurÃ³', 'asÃ­', 'atras', 'aÃºn', 'aunque', 'ayer',
    'bajo', 'bastante', 'bien', 'buen', 'buena', 'buenas', 'bueno', 'buenos',
    'cada', 'casi', 'cerca', 'cierta', 'ciertas', 'cierto', 'ciertos', 'cinco',
    'comentÃ³', 'como', 'cÃ³mo', 'con', 'conocer', 'conseguimos', 'conseguir',
    'considera', 'considerÃ³', 'consigo', 'consigue', 'consiguen', 'consigues',
    'contra', 'cosas', 'creo', 'cual', 'cuales', 'cualquier', 'cuando',
    'cuanto', 'cuatro', 'cuenta', 'da', 'dado', 'dan', 'dar', 'de', 'debe',
    'deben', 'debido', 'decir', 'dejÃ³', 'del', 'demÃ¡s', 'dentro', 'desde',
    'despuÃ©s', 'dice', 'dicen', 'dicho', 'dieron', 'diferente', 'diferentes',
    'dijeron', 'dijo', 'dio', 'donde', 'dos', 'durante', 'e', 'ejemplo', 'el',
    'de', 'la', 'el', 'porfas', 't', 'p', 'd', 'est',
    'Ã©l', 'ella', 'ellas', 'ello', 'ellos', 'embargo', 'empleais', 'emplean',
    'emplear', 'empleas', 'empleo', 'en', 'encima', 'encuentra', 'entonces',
    'entre', 'era', 'eramos', 'eran', 'eras', 'eres', 'es', 'esa', 'esas',
    'ese', 'eso', 'esos', 'esta', 'Ã©sta', 'estÃ¡', 'estaba', 'estaban',
    'estado', 'estais', 'estamos', 'estan', 'estÃ¡n', 'estar', 'estarÃ¡',
    'estas', 'Ã©stas', 'este', 'Ã©ste', 'esto', 'estos', 'Ã©stos', 'estoy',
    'estuvo', 'ex', 'existe', 'existen', 'explicÃ³', 'expresÃ³', 'fin', 'fue',
    'fuera', 'fueron', 'fui', 'fuimos', 'gran', 'grandes', 'gueno', 'ha',
    'haber', 'habÃ­a', 'habÃ­an', 'habrÃ¡', 'hace', 'haceis', 'hacemos', 'hacen',
    'hacer', 'hacerlo', 'haces', 'hacia', 'haciendo', 'hago', 'han', 'hasta',
    'hay', 'haya', 'he', 'hecho', 'hemos', 'hicieron', 'hizo', 'hoy', 'hubo',
    'igual', 'incluso', 'indicÃ³', 'informÃ³', 'intenta', 'intentais',
    'intentamos', 'intentan', 'intentar', 'intentas', 'intento', 'ir', 'junto',
    'la', 'lado', 'largo', 'las', 'le', 'les', 'llegÃ³', 'lleva', 'llevar',
    'lo', 'los', 'luego', 'lugar', 'manera', 'manifestÃ³', 'mÃ¡s', 'mayor', 'me',
    'mediante', 'mejor', 'mencionÃ³', 'menos', 'mi', 'mientras', 'mio', 'misma',
    'mismas', 'mismo', 'mismos', 'modo', 'momento', 'mucha', 'muchas', 'mucho',
    'muchos', 'muy', 'nada', 'nadie', 'ni', 'ningÃºn', 'ninguna', 'ningunas',
    'ninguno', 'ningunos', 'nos', 'nosotras', 'nosotros', 'nuestra',
    'nuestras', 'nuestro', 'nuestros', 'nueva', 'nuevas', 'nuevo', 'nuevos',
    'nunca', 'o', 'ocho', 'otra', 'otras', 'otro', 'otros', 'para', 'parece',
    'parte', 'partir', 'pasada', 'pasado', 'pero', 'pesar', 'poca', 'pocas',
    'poco', 'pocos', 'podeis', 'podemos', 'poder', 'podrÃ¡', 'podrÃ¡n', 'podria',
    'podrÃ­a', 'podriais', 'podriamos', 'podrian', 'podrÃ­an', 'podrias',
    'poner', 'por', 'porque', 'por quÃ©', 'posible', 'primer', 'primera',
    'primero', 'primeros', 'principalmente', 'propia', 'propias', 'propio',
    'propios', 'prÃ³ximo', 'prÃ³ximos', 'pudo', 'pueda', 'puede', 'pueden',
    'puedo', 'pues', 'que', 'quÃ©', 'quedÃ³', 'queremos', 'quien', 'quiÃ©n',
    'quienes', 'quiere', 'realizado', 'realizar', 'realizÃ³', 'respecto',
    'sabe', 'sabeis', 'sabemos', 'saben', 'saber', 'sabes', 'se', 'sea',
    'sean', 'segÃºn', 'segunda', 'segundo', 'seis', 'seÃ±alÃ³', 'ser', 'serÃ¡',
    'serÃ¡n', 'serÃ­a', 'si', 'sÃ­', 'sido', 'siempre', 'siendo', 'siete',
    'sigue', 'siguiente', 'sin', 'sino', 'sobre', 'sois', 'sola', 'solamente',
    'solas', 'solo', 'sÃ³lo', 'solos', 'somos', 'son', 'soy', 'su', 'sus',
    'tal', 'tambiÃ©n', 'tampoco', 'tan', 'tanto', 'tendrÃ¡', 'tendrÃ¡n', 'teneis',
    'tenemos', 'tener', 'tenga', 'tengo', 'tenÃ­a', 'tenido', 'tercera',
    'tiempo', 'tiene', 'tienen', 'toda', 'todas', 'todavÃ­a', 'todo', 'todos',
    'total', 'trabaja', 'trabajais', 'trabajamos', 'trabajan', 'trabajar',
    'trabajas', 'trabajo', 'tras', 'trata', 'travÃ©s', 'tres', 'tuvo', 'tuyo',
    'tu', 'te', 'pq', 'mas', 'qie', 'us', 'has', 'ti', 'ahi', 'mis', 'tus',
    'do', 'X', 'Ven', 'mo', 'Don', 'dia', 'PT', 'sua', 'q', 'x', 'i', 
    'Ãºltima', 'Ãºltimas', 'ultimo', 'Ãºltimo', 'Ãºltimos', 'un', 'una', 'unas',
    'uno', 'unos', 'usa', 'usais', 'usamos', 'usan', 'usar', 'usas', 'uso',
    'usted', 'va', 'vais', 'valor', 'vamos', 'van', 'varias', 'varios', 'vaya',
    'veces', 'ver', 'verdad', 'verdadera', 'verdadero', 'vez', 'vosotras',
    'n', 's', 'of', 'c', 'the', 'm', 'qu', 'to', 'as', 'is',
    'asi', 'via', 'sera', 'tambien', 'vosotros', 'voy', 'y', 'ya', 'yo']

def text_to_wordlist(text, remove_stop_words=True, stem_words=False):
    # limpiar el  texto
    text = str(text).strip()
    #remplazar los acentos
    for s,t in DIACRITICAL_VOWELS:
        text = re.sub(r'{0}'.format(s), t, text)
   #remplazar el SLANG
    for s,t in SLANG:
        text = re.sub(r'\b{0}\b'.format(s), t, text)
        
   # for s,t in SIMBOLOS_POSITIVOS:
   #     text = re.sub(r'\b{0}\b'.format(s), t, text)
   # for s,t in SIMBOLOS_NEGATIVOS:
   #    text = re.sub(r'\b{0}\b'.format(s), t, text)

    text = re.sub(r"[^A-Za-z0-9]", " ", text)
    text = re.sub(r"\'s", " ", text)
    text = re.sub(r"\0s", "0", text)
    text = re.sub(r" 9 11 ", "911", text)
    text = re.sub(r"e-mail", "email", text)
    text = re.sub(r"\0rs ", " rs ", text) 
    text = re.sub(r"gps", "GPS", text)
    text = re.sub(r"gst", "GST", text)
   #convertir a minusculas
    text = text.lower()
    # Remove punctuation from text
    text = ''.join([c for c in text if c not in punctuation])

    # remover stop words
    if remove_stop_words:
        text = text.split()
        text = [w for w in text if not w in stop_words]
        text = " ".join(text)
    # Optionally, shorten words to their stems
    #if stem_words:
    #    text = text.split()
    #    stemmer = SnowballStemmer('spanish')
    #    stemmed_words = [stemmer.stem(word) for word in text]
    #    text = " ".join(stemmed_words)

    # Return a list of words
    return(text)

data=dataset

from sklearn.model_selection import train_test_split
np.random.seed(4)
data, data_test = train_test_split(data,  test_size=0.2, random_state=42)

data_test['label'] = data['POS'].map({"N":0, "NEU":1, "P":2, "NN":0})
data_test['label'].value_counts()

question_list = list()
for question in dataset.full_text:
  question_list.append(text_to_wordlist(str(question).strip()))
df1 = pd.DataFrame(question_list, columns =['full_text']) 
df1["polaridad"] = dataset['POS'].tolist()

"""**TRAIN**"""

dataframe=spark.createDataFrame(df1, ["text", "category"])\
               .filter(func.col("category").isin(["N", "P","NEU"]))\
               .withColumn("id", func.monotonically_increasing_id())\
               .withColumn("label",  (func.when(func.col("category")=="N",0)\
                                    .when(func.col("category")=="P",2)\
                                     .otherwise(1)).cast("double"))\
              .dropDuplicates()

dataframe.show()

id_columns = ["id"]
categorial_columns = ["text"]
numerical_columns = []
label_columns = ["label"]



#naive Bayes
train_data, valid_data = dataframe.randomSplit([0.7, 0.3])
tokenizer = Tokenizer(inputCol="text", outputCol="words")
hashingTF = HashingTF(inputCol="words", outputCol="features")

nb = NaiveBayes(labelCol="label", featuresCol="features")
nb_pipeline = Pipeline(stages=[tokenizer, hashingTF, nb])

model = nb_pipeline.fit(train_data)
pr = model.transform(valid_data)

nb_evaluator = get_multiclass_evaluator(pr)


nb_paramGrid = (ParamGridBuilder()
               .addGrid(nb.smoothing, [0.0, 0.2, 0.4, 0.6, 0.8, 1.0])
               .build())


nb_crossval = CrossValidator(estimator=nb_pipeline, 
                             estimatorParamMaps=nb_paramGrid, 
                             evaluator=nb_evaluator, 
                             numFolds=5)
nb_cv_model = nb_crossval.fit(train_data)
nb_cv_prediction = nb_cv_model.bestModel.transform(valid_data)




nb_cv_model.bestModel.write().overwrite().save("data/model/CVModel_NB")
#nb_cv_model.bestModel.write().overwrite().save("drive/My Drive/model/CVModel_NB")

#train_data.to_csv('data.csv')
#!cp data.csv "drive/My Drive/"

nb_cv_multiclass_evaluator = get_multiclass_evaluator(nb_cv_prediction)

get_tunning_hiperparameter(nb_paramGrid, nb_cv_model)

plot_matrix_confusion(nb_cv_prediction)

plot_roc_curve2(nb_cv_prediction)

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
# %matplotlib inline
#Grafico de la curva Roc
plt.plot(espc_1, sens, marker="o", linestyle="--", color="b")
x=[i*0.01 for i in range(100)]
y=[i*0.01 for i in range(100)]
plt.plot(x,y)
plt.xlabel("True Positive Rate")
plt.ylabel("False Positive Rate")
plt.title("Curva ROC")

"""**TEST**"""

df1_test = df1.copy()
del df1_test["polaridad"]

#Donde N=0 y P=1
dataframe=spark.createDataFrame(df1_test, ["text"])\
               .withColumn("id", func.monotonically_increasing_id())\
               .dropDuplicates()

loadedPipeline = PipelineModel.read().load("data/model/CVModel_NB")
predictions = loadedPipeline.transform(dataframe)

predictions.show(200)

from google.colab import drive
drive.mount('/content/drive')

dataframe

