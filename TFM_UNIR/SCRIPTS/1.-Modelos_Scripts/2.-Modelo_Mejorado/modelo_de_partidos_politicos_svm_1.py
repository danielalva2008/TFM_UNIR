# -*- coding: utf-8 -*-
"""Modelo_de_Partidos_PoliticosSVM_v2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Wj-7YPYem_jgLYiKE9UnRMgzRwhU-SNc

# Fase 1: Importar las dependencias
"""

#instalamos las librerias
!pip3 install clean-text[gpl]
!pip3 install py4j
!pip3 install -q findspark
!pip3 install tqdm 
!pip3 install plotly 
!pip3 install pyspark==2.4.0

!apt-get update

#@title Texto de título predeterminado
!sudo apt-get install openjdk-8-jdk-headless -qq > /dev/null
!wget -q https://www.dropbox.com/s/96rr4avobbz9a1p/spark-2.4.0-bin-hadoop2.7.tar
!tar xf spark-2.4.0-bin-hadoop2.7.tar

# Commented out IPython magic to ensure Python compatibility.
#importamos la librerias necesarias para el preprocesamiento
# %matplotlib inline
from nltk import TweetTokenizer
from sklearn.metrics import roc_curve,auc
import pandas as pd
import numpy as np
import nltk
from nltk.corpus import stopwords
from nltk.stem import SnowballStemmer
import re
from string import punctuation
from cleantext import clean
import os
import matplotlib.pyplot as plt
from gensim.models import KeyedVectors
from sklearn.datasets import fetch_20newsgroups

from pyspark.sql import SparkSession
from pyspark import SparkContext
from pyspark.sql.types import *
from pyspark.sql import *
from pyspark.sql import Row
from pyspark.sql import functions as func
from pyspark.mllib.regression import LabeledPoint
from pyspark.mllib.regression import LinearRegressionModel
from pyspark.ml.classification import RandomForestClassifier
from pyspark.ml.feature import  SQLTransformer
from pyspark.ml.evaluation import  BinaryClassificationEvaluator
from pyspark.ml.classification import NaiveBayes
from pyspark.mllib.evaluation import MulticlassMetrics
from pyspark.ml.evaluation import MulticlassClassificationEvaluator
from pyspark.ml.feature import (VectorAssembler,StringIndexer,StandardScaler,
                                OneHotEncoderEstimator,OneHotEncoder, 
                                VectorIndexer, IndexToString,
                                HashingTF, Tokenizer)
from pyspark.ml.tuning import ParamGridBuilder, CrossValidator
from pyspark.ml import Pipeline, PipelineModel
from pyspark.sql import Row
from pyspark.ml.classification import LinearSVC,OneVsRest,LogisticRegression
from pyspark.ml.clustering import KMeans
from pyspark.ml.evaluation import ClusteringEvaluator

import os
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"
os.environ["SPARK_HOME"] = "spark-2.4.0-bin-hadoop2.7"

import findspark
findspark.init("spark-2.4.0-bin-hadoop2.7")

"""# Fase 2: Pre Procesado de Datos"""

# !wget https://www.dropbox.com/s/o8eq5kzpcl7ejpv/AlDataset.csv
# !wget https://www.dropbox.com/s/oeemdkwkqvlb1rk/PartidoMoradoPEtrain3.csv
# !wget https://www.dropbox.com/s/41errlqkbihmcc8/AlDatasetTassF.csv
# !wget https://www.dropbox.com/s/bvx5gozhfur6igo/AlDataset3.csv
!wget https://www.dropbox.com/s/np2v706jyz4yz44/data_extended.csv

# funcion para la matriz de confusion
def plot_matrix_confusion(lr_cv_predictions,
                          normalize=False,
                          title='Confusion matrix',
                          cmap=plt.cm.Blues):
  from numpy import array
  import itertools
  
  class_temp = lr_cv_predictions.select("label").groupBy("label")\
                              .count().sort('count', ascending=False).toPandas()
  class_temp = class_temp["label"].tolist()
  class_names = list(map(str, class_temp))
  
    
  predictions_and_labels = lr_cv_predictions.select("prediction", "label").rdd.map(lambda r: (float(r[0]), float(r[1])))
  metrics = MulticlassMetrics(predictions_and_labels)
  confusion_matrix = metrics.confusionMatrix().toArray()
  met_list = array([[int(x[0]), int(x[1]),int(x[2])] for x in confusion_matrix])

    
  cm = met_list
  classes = class_names
  if normalize:
      cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]

  plt.imshow(cm, interpolation='nearest', cmap=cmap)
  plt.title(title)
  plt.colorbar()
  tick_marks = np.arange(len(classes))
  plt.xticks(tick_marks, classes, rotation=45)
  plt.yticks(tick_marks, classes)

  fmt = '.2f' if normalize else 'd'
  thresh = cm.max() / 2.
  for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
      plt.text(j, i, format(cm[i, j], fmt),
               horizontalalignment="center",
               color="white" if cm[i, j] > thresh else "black")

  plt.tight_layout()
  plt.ylabel('True label')
  plt.xlabel('Predicted label')
  plt.show()

def plot_roc_curve2(cv_prediction):
  #PREDICTIONS
  predictions_pddf = cv_prediction.select('prediction','label').toPandas()
  prob = predictions_pddf["prediction"] 
  fpr, tpr, thresholds = roc_curve(predictions_pddf['label'], prob, pos_label=1);
  roc_auc = auc(fpr, tpr)

  # PLOT ROC CURVES
  plt.figure(figsize=(5,5))
  plt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % roc_auc)
  plt.plot([0, 1], [0, 1], 'k--')
  plt.xlim([0.0, 1.0])
  plt.ylim([0.0, 1.05])
  plt.xlabel('False Positive Rate')
  plt.ylabel('True Positive Rate')
  plt.title('ROC Curve')
  plt.legend(loc="lower right")
  plt.show()

def get_multiclass_evaluator(prediction):
  lr_multi_evaluator = MulticlassClassificationEvaluator(labelCol='label', predictionCol='prediction')

  # Accuracy
  lr_accuracy = lr_multi_evaluator.evaluate(prediction, {lr_multi_evaluator.metricName: "accuracy"})
  print("Accuracy: {:.2f}".format(lr_accuracy))

  # f1
  f1 = lr_multi_evaluator.evaluate(prediction, {lr_multi_evaluator.metricName: "f1"})
  print("f1: {:.2f}".format(f1))

  # Precision
  wp = lr_multi_evaluator.evaluate(prediction, {lr_multi_evaluator.metricName: "weightedPrecision"})
  print("Precision: {:.2f}".format(wp))

  # Recall
  wr = lr_multi_evaluator.evaluate(prediction, {lr_multi_evaluator.metricName: "weightedRecall"})
  print("Recall: {:.2f}".format(wr))
  
  return lr_multi_evaluator

def get_tunning_hiperparameter(grid, cv_model):
  bestPipeline = cv_model.bestModel
  bestLRModel = bestPipeline.stages[-1]
  result = {param.name: bestLRModel.getOrDefault(bestLRModel.getParam(param.name)) for param in grid[0]}
  return result

spark = SparkSession.builder.master("local[*]").appName("Pricing").getOrCreate()

path ="data_extended.csv"

train_data = pd.read_csv(
    path,
   error_bad_lines=False,
)

dataset=train_data
dataset

DIACRITICAL_VOWELS = [('á','a'), ('é','e'), ('í','i'), ('ó','o'), ('ú','u'), ('ü','u')]
SLANG = [('d','de'), ('[qk]','que'), ('xo','pero'), ('xa', 'para'), ('[xp]q','porque'),('es[qk]', 'es que'),
              ('fvr','favor'),('(xfa|xf|pf|plis|pls|porfa)', 'por favor'), ('dnd','donde'), ('tb', 'también'),
              ('(tq|tk)', 'te quiero'), ('(tqm|tkm)', 'te quiero mucho'), ('x','por'), ('\+','mas'),
              ('piña','mala suerte'),('agarre','adulterio'),('ampay','verguenza'),('bacan','alegria'),
              ('bamba','falsificado'),('cabeceador','ladron'),('cabro','homosexual'),('cachaciento','burlon'),
              ('calabacita','tonta'),('caleta','secreto'),('cabro','homosexual'),('cana','carcel'),
              ('chucha','molestia'),('choro','ladron'),('conchán','conchudo'),('cutra','ilicito'),
              ('dark','horrible'),('lenteja','torpe'),('lorna','tonto'),('mancar','morir'),
              ('monse','tonto'),('lenteja','torpe'),('lorna','tonto'),('mancar','morir'),('piñata','mala suerte')
              ]

stop_words=['a', 'actualmente', 'adelante', 'además', 'afirmó', 'agregó', 'ahí', 'ahora',
    'cc', 'pa', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l',
    'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'al',
    'algo', 'algún', 'algún', 'alguna', 'algunas', 'alguno', 'algunos',
    'alrededor', 'ambos', 'ampleamos', 'añadió', 'ante', 'anterior', 'antes',
    'apenas', 'aproximadamente', 'aquel', 'aquellas', 'aquellos', 'aqui',
    'aquí', 'arriba', 'aseguró', 'así', 'atras', 'aún', 'aunque', 'ayer',
    'bajo', 'bastante', 'bien', 'buen', 'buena', 'buenas', 'bueno', 'buenos',
    'cada', 'casi', 'cerca', 'cierta', 'ciertas', 'cierto', 'ciertos', 'cinco',
    'comentó', 'como', 'cómo', 'con', 'conocer', 'conseguimos', 'conseguir',
    'considera', 'consideró', 'consigo', 'consigue', 'consiguen', 'consigues',
    'contra', 'cosas', 'creo', 'cual', 'cuales', 'cualquier', 'cuando',
    'cuanto', 'cuatro', 'cuenta', 'da', 'dado', 'dan', 'dar', 'de', 'debe',
    'deben', 'debido', 'decir', 'dejó', 'del', 'demás', 'dentro', 'desde',
    'después', 'dice', 'dicen', 'dicho', 'dieron', 'diferente', 'diferentes',
    'dijeron', 'dijo', 'dio', 'donde', 'dos', 'durante', 'e', 'ejemplo', 'el',
    'de', 'la', 'el', 'porfas', 't', 'p', 'd', 'est',
    'él', 'ella', 'ellas', 'ello', 'ellos', 'embargo', 'empleais', 'emplean',
    'emplear', 'empleas', 'empleo', 'en', 'encima', 'encuentra', 'entonces',
    'entre', 'era', 'eramos', 'eran', 'eras', 'eres', 'es', 'esa', 'esas',
    'ese', 'eso', 'esos', 'esta', 'ésta', 'está', 'estaba', 'estaban',
    'estado', 'estais', 'estamos', 'estan', 'están', 'estar', 'estará',
    'estas', 'éstas', 'este', 'éste', 'esto', 'estos', 'éstos', 'estoy',
    'estuvo', 'ex', 'existe', 'existen', 'explicó', 'expresó', 'fin', 'fue',
    'fuera', 'fueron', 'fui', 'fuimos', 'gran', 'grandes', 'gueno', 'ha',
    'haber', 'había', 'habían', 'habrá', 'hace', 'haceis', 'hacemos', 'hacen',
    'hacer', 'hacerlo', 'haces', 'hacia', 'haciendo', 'hago', 'han', 'hasta',
    'hay', 'haya', 'he', 'hecho', 'hemos', 'hicieron', 'hizo', 'hoy', 'hubo',
    'igual', 'incluso', 'indicó', 'informó', 'intenta', 'intentais',
    'intentamos', 'intentan', 'intentar', 'intentas', 'intento', 'ir', 'junto',
    'la', 'lado', 'largo', 'las', 'le', 'les', 'llegó', 'lleva', 'llevar',
    'lo', 'los', 'luego', 'lugar', 'manera', 'manifestó', 'más', 'mayor', 'me',
    'mediante', 'mejor', 'mencionó', 'menos', 'mi', 'mientras', 'mio', 'misma',
    'mismas', 'mismo', 'mismos', 'modo', 'momento', 'mucha', 'muchas', 'mucho',
    'muchos', 'muy', 'nada', 'nadie', 'ni', 'ningún', 'ninguna', 'ningunas',
    'ninguno', 'ningunos', 'nos', 'nosotras', 'nosotros', 'nuestra',
    'nuestras', 'nuestro', 'nuestros', 'nueva', 'nuevas', 'nuevo', 'nuevos',
    'nunca', 'o', 'ocho', 'otra', 'otras', 'otro', 'otros', 'para', 'parece',
    'parte', 'partir', 'pasada', 'pasado', 'pero', 'pesar', 'poca', 'pocas',
    'poco', 'pocos', 'podeis', 'podemos', 'poder', 'podrá', 'podrán', 'podria',
    'podría', 'podriais', 'podriamos', 'podrian', 'podrían', 'podrias',
    'poner', 'por', 'porque', 'por qué', 'posible', 'primer', 'primera',
    'primero', 'primeros', 'principalmente', 'propia', 'propias', 'propio',
    'propios', 'próximo', 'próximos', 'pudo', 'pueda', 'puede', 'pueden',
    'puedo', 'pues', 'que', 'qué', 'quedó', 'queremos', 'quien', 'quién',
    'quienes', 'quiere', 'realizado', 'realizar', 'realizó', 'respecto',
    'sabe', 'sabeis', 'sabemos', 'saben', 'saber', 'sabes', 'se', 'sea',
    'sean', 'según', 'segunda', 'segundo', 'seis', 'señaló', 'ser', 'será',
    'serán', 'sería', 'si', 'sí', 'sido', 'siempre', 'siendo', 'siete',
    'sigue', 'siguiente', 'sin', 'sino', 'sobre', 'sois', 'sola', 'solamente',
    'solas', 'solo', 'sólo', 'solos', 'somos', 'son', 'soy', 'su', 'sus',
    'tal', 'también', 'tampoco', 'tan', 'tanto', 'tendrá', 'tendrán', 'teneis',
    'tenemos', 'tener', 'tenga', 'tengo', 'tenía', 'tenido', 'tercera',
    'tiempo', 'tiene', 'tienen', 'toda', 'todas', 'todavía', 'todo', 'todos',
    'total', 'trabaja', 'trabajais', 'trabajamos', 'trabajan', 'trabajar',
    'trabajas', 'trabajo', 'tras', 'trata', 'través', 'tres', 'tuvo', 'tuyo',
    'tu', 'te', 'pq', 'mas', 'qie', 'us', 'has', 'ti', 'ahi', 'mis', 'tus',
    'do', 'X', 'Ven', 'mo', 'Don', 'dia', 'PT', 'sua', 'q', 'x', 'i', 
    'última', 'últimas', 'ultimo', 'último', 'últimos', 'un', 'una', 'unas',
    'uno', 'unos', 'usa', 'usais', 'usamos', 'usan', 'usar', 'usas', 'uso',
    'usted', 'va', 'vais', 'valor', 'vamos', 'van', 'varias', 'varios', 'vaya',
    'veces', 'ver', 'verdad', 'verdadera', 'verdadero', 'vez', 'vosotras',
    'n', 's', 'of', 'c', 'the', 'm', 'qu', 'to', 'as', 'is',
    'asi', 'via', 'sera', 'tambien', 'vosotros', 'voy', 'y', 'ya', 'yo']

def text_to_wordlist(text, remove_stop_words=True, stem_words=False):
    # limpiar el  texto
    text = str(text).strip()
    text = re.sub(r"http\S+", "", text)
    text = re.sub(r"#\S+", "", text)
    #remplazar los acentos
    for s,t in DIACRITICAL_VOWELS:
        text = re.sub(r'{0}'.format(s), t, text)
   #remplazar el SLANG
    for s,t in SLANG:
        text = re.sub(r'\b{0}\b'.format(s), t, text)
    text = re.sub(r"@[A-Za-z0-9]+", ' ', text)
    text = re.sub(r"[^A-Za-z0-9]", " ", text)
    text = re.sub(r"\'s", " ", text)
    text = re.sub(r"\0s", "0", text)
    text = re.sub(r" 9 11 ", "911", text)
    text = re.sub(r"e-mail", "email", text)
    text = re.sub(r"\0rs ", " rs ", text) 
    text = re.sub(r"gps", "GPS", text)
    text = re.sub(r"gst", "GST", text)
   #convertir a minusculas
    text = text.lower()
    # Remove punctuation from text
    text = ''.join([c for c in text if c not in punctuation ])

    # remover stop words
    if remove_stop_words:
        text = text.split()
        text = [w for w in text if not w in stop_words and len(w)>2]
        text = " ".join(text)
    text = re.sub(r' {2,}' , ' ', text)
    return(text.strip())

question_list = list()
for question in dataset.full_text:
  question_list.append(text_to_wordlist(str(question).strip()))
df1 = pd.DataFrame(question_list, columns =['full_text']) 
df1["polaridad"] = dataset['POS'].tolist()

df1

df1.polaridad.value_counts()

df1.loc[df1.polaridad == "NN", "polaridad"] = "N"

df1.polaridad = df1.polaridad.apply(lambda x: str(x).strip().upper())
df1.polaridad.value_counts()

"""Removemos valores duplicados"""

df1.drop_duplicates(inplace=True)

"""Removemos valores faltantes, si los hubiera"""

df1.fillna(value='', inplace=True)

df1.dropna(inplace=True)

df1.shape

"""# Fase 3: Construción del modelo

---


"""

dataframe=spark.createDataFrame(df1, ["text", "category"])\
               .filter(func.col("category").isin(["N", "P","NEU"]))\
               .withColumn("id", func.monotonically_increasing_id())\
               .withColumn("label",  (func.when(func.col("category")=="N",0)\
                                    .when(func.col("category")=="P",2)\
                                     .otherwise(1)).cast("double"))\
              .dropDuplicates()

print((dataframe.count(), len(dataframe.columns)))

dataframe.show()

id_columns = ["id"]
categorial_columns = ["text"]
numerical_columns = []
label_columns = ["label"]

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# train_data, valid_data = dataframe.randomSplit([0.7, 0.3])
# 
# tokenizer = Tokenizer(inputCol="text", outputCol="words")
# 
# hashingTF = HashingTF(inputCol="words", outputCol="features")
# 
# svm = LinearSVC(labelCol="label", featuresCol="features", maxIter= 1)
# 
# ovr = OneVsRest(classifier=svm)
# 
# svm_pipeline = Pipeline(stages=[tokenizer, hashingTF, ovr])
# 
# ovrModel = svm_pipeline.fit(train_data)
# 
# predictions = ovrModel.transform(valid_data)

train_data.show(2)

svm_evaluator = get_multiclass_evaluator(predictions)

plot_matrix_confusion(predictions)

predictions.show()

"""## Busqueda de hiperparámetros"""

svm_paramGrid = (ParamGridBuilder()
               .addGrid(svm.maxIter, [1000])
               .build())

from itertools import product

max_iter_list, reg_param_list = [1000], [.0, .1, .5, 1.0]
comb_output = list(product(max_iter_list, reg_param_list))
comb_output

# Commented out IPython magic to ensure Python compatibility.
# %%time
# for regParam in reg_param_list:
#     svm = LinearSVC(labelCol="label", featuresCol="features",maxIter= 1,regParam= regParam)
#     ovr = OneVsRest(classifier=svm)
#     svm_pipeline = Pipeline(stages=[tokenizer, hashingTF, ovr])
#     svm_crossval = CrossValidator(estimator=svm_pipeline, 
#                                  estimatorParamMaps=svm_paramGrid, 
#                                  evaluator=svm_evaluator, 
#                                  numFolds=2)
#     svm_cv_model = svm_crossval.fit(train_data)
#     svm_cv_prediction = svm_cv_model.bestModel.transform(valid_data)
#     get_multiclass_evaluator(svm_cv_prediction)
#     svm_cv_model.bestModel.write().overwrite().save("data/model/CVModel_SVM_{}_{}".format('all', regParam))

"""El mejor modelo es que tiene el valor de regulación 'regParam' igual a 0.1"""

!ls "data/model/"

"""## Guardamos el mejor modelo"""

!zip -r models_svm.zip data/

from google.colab import files
files.download("models_svm.zip")

loadPipeline = PipelineModel.read().load("data/model/CVModel_SVM_all_0.1")
svm_cv_prediction = loadPipeline.transform(valid_data)

"""##Model Evaluation"""

svm_cv_multiclass_evaluator = get_multiclass_evaluator(svm_cv_prediction)

plot_matrix_confusion(svm_cv_prediction)

plot_roc_curve2(svm_cv_prediction)

"""# Paso 4: Aplicación y Evaluación

"""

df1_test = df1.copy()
del df1_test["polaridad"]

dataframe=spark.createDataFrame(df1_test, ["text"])\
               .withColumn("id", func.monotonically_increasing_id())\
               .dropDuplicates()

predictions = loadPipeline.transform(dataframe)

predictions.show(200)

