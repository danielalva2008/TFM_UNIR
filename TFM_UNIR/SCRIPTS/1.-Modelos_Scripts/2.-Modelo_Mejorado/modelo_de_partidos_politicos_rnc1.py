# -*- coding: utf-8 -*-
"""Modelo_de_Partidos_Politicos_RNC1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1X0JNUXS56hnn_TUBPONne0iuCit8pmmB

# Fase 1: Importar las dependencias
"""

import numpy as np
import math
import re
import pandas as pd
from bs4 import BeautifulSoup
from google.colab import drive
from string import punctuation

# Commented out IPython magic to ensure Python compatibility.
try:
#     %tensorflow_version 2.x
except Exception:
    pass
import tensorflow as tf
tf.version
from tensorflow.keras import layers
import tensorflow_datasets as tfds

import urllib.request
from bs4 import BeautifulSoup as soup
import re
import random
from tensorflow.keras.callbacks import EarlyStopping

"""# Fase 2: Pre Procesado de Datos

## Carga de Ficheros
"""

!wget https://www.dropbox.com/s/np2v706jyz4yz44/data_extended.csv

train_data = pd.read_csv(
    "data_extended.csv",
)

train_data.tail(5)


data = train_data

"""## Pre Procesado

### Limpieza
"""

data.shape

DIACRITICAL_VOWELS = [('á','a'), ('é','e'), ('í','i'), ('ó','o'), ('ú','u'), ('ü','u')]
SLANG = [('d','de'), ('[qk]','que'), ('xo','pero'), ('xa', 'para'), ('[xp]q','porque'),('es[qk]', 'es que'),
              ('fvr','favor'),('(xfa|xf|pf|plis|pls|porfa)', 'por favor'), ('dnd','donde'), ('tb', 'también'),
              ('(tq|tk)', 'te quiero'), ('(tqm|tkm)', 'te quiero mucho'), ('x','por'), ('\+','mas'),
              ('piña','mala suerte'),('agarre','adulterio'),('ampay','verguenza'),('bacan','alegria'),
              ('bamba','falsificado'),('cabeceador','ladron'),('cabro','homosexual'),('cachaciento','burlon'),
              ('calabacita','tonta'),('caleta','secreto'),('cabro','homosexual'),('cana','carcel'),
              ('chucha','molestia'),('choro','ladron'),('conchán','conchudo'),('cutra','ilicito'),
              ('dark','horrible'),('lenteja','torpe'),('lorna','tonto'),('mancar','morir'),
              ('monse','tonto'),('lenteja','torpe'),('lorna','tonto'),('mancar','morir'),('piñata','mala suerte')
              ]

stop_words=['a', 'actualmente', 'adelante', 'además', 'afirmó', 'agregó', 'ahí', 'ahora',
    'cc', 'pa', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l',
    'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'al',
    'algo', 'algún', 'algún', 'alguna', 'algunas', 'alguno', 'algunos',
    'alrededor', 'ambos', 'ampleamos', 'añadió', 'ante', 'anterior', 'antes',
    'apenas', 'aproximadamente', 'aquel', 'aquellas', 'aquellos', 'aqui',
    'aquí', 'arriba', 'aseguró', 'así', 'atras', 'aún', 'aunque', 'ayer',
    'bajo', 'bastante', 'bien', 'buen', 'buena', 'buenas', 'bueno', 'buenos',
    'cada', 'casi', 'cerca', 'cierta', 'ciertas', 'cierto', 'ciertos', 'cinco',
    'comentó', 'como', 'cómo', 'con', 'conocer', 'conseguimos', 'conseguir',
    'considera', 'consideró', 'consigo', 'consigue', 'consiguen', 'consigues',
    'contra', 'cosas', 'creo', 'cual', 'cuales', 'cualquier', 'cuando',
    'cuanto', 'cuatro', 'cuenta', 'da', 'dado', 'dan', 'dar', 'de', 'debe',
    'deben', 'debido', 'decir', 'dejó', 'del', 'demás', 'dentro', 'desde',
    'después', 'dice', 'dicen', 'dicho', 'dieron', 'diferente', 'diferentes',
    'dijeron', 'dijo', 'dio', 'donde', 'dos', 'durante', 'e', 'ejemplo', 'el',
    'de', 'la', 'el', 'porfas', 't', 'p', 'd', 'est',
    'él', 'ella', 'ellas', 'ello', 'ellos', 'embargo', 'empleais', 'emplean',
    'emplear', 'empleas', 'empleo', 'en', 'encima', 'encuentra', 'entonces',
    'entre', 'era', 'eramos', 'eran', 'eras', 'eres', 'es', 'esa', 'esas',
    'ese', 'eso', 'esos', 'esta', 'ésta', 'está', 'estaba', 'estaban',
    'estado', 'estais', 'estamos', 'estan', 'están', 'estar', 'estará',
    'estas', 'éstas', 'este', 'éste', 'esto', 'estos', 'éstos', 'estoy',
    'estuvo', 'ex', 'existe', 'existen', 'explicó', 'expresó', 'fin', 'fue',
    'fuera', 'fueron', 'fui', 'fuimos', 'gran', 'grandes', 'gueno', 'ha',
    'haber', 'había', 'habían', 'habrá', 'hace', 'haceis', 'hacemos', 'hacen',
    'hacer', 'hacerlo', 'haces', 'hacia', 'haciendo', 'hago', 'han', 'hasta',
    'hay', 'haya', 'he', 'hecho', 'hemos', 'hicieron', 'hizo', 'hoy', 'hubo',
    'igual', 'incluso', 'indicó', 'informó', 'intenta', 'intentais',
    'intentamos', 'intentan', 'intentar', 'intentas', 'intento', 'ir', 'junto',
    'la', 'lado', 'largo', 'las', 'le', 'les', 'llegó', 'lleva', 'llevar',
    'lo', 'los', 'luego', 'lugar', 'manera', 'manifestó', 'más', 'mayor', 'me',
    'mediante', 'mejor', 'mencionó', 'menos', 'mi', 'mientras', 'mio', 'misma',
    'mismas', 'mismo', 'mismos', 'modo', 'momento', 'mucha', 'muchas', 'mucho',
    'muchos', 'muy', 'nada', 'nadie', 'ni', 'ningún', 'ninguna', 'ningunas',
    'ninguno', 'ningunos', 'nos', 'nosotras', 'nosotros', 'nuestra',
    'nuestras', 'nuestro', 'nuestros', 'nueva', 'nuevas', 'nuevo', 'nuevos',
    'nunca', 'o', 'ocho', 'otra', 'otras', 'otro', 'otros', 'para', 'parece',
    'parte', 'partir', 'pasada', 'pasado', 'pero', 'pesar', 'poca', 'pocas',
    'poco', 'pocos', 'podeis', 'podemos', 'poder', 'podrá', 'podrán', 'podria',
    'podría', 'podriais', 'podriamos', 'podrian', 'podrían', 'podrias',
    'poner', 'por', 'porque', 'por qué', 'posible', 'primer', 'primera',
    'primero', 'primeros', 'principalmente', 'propia', 'propias', 'propio',
    'propios', 'próximo', 'próximos', 'pudo', 'pueda', 'puede', 'pueden',
    'puedo', 'pues', 'que', 'qué', 'quedó', 'queremos', 'quien', 'quién',
    'quienes', 'quiere', 'realizado', 'realizar', 'realizó', 'respecto',
    'sabe', 'sabeis', 'sabemos', 'saben', 'saber', 'sabes', 'se', 'sea',
    'sean', 'según', 'segunda', 'segundo', 'seis', 'señaló', 'ser', 'será',
    'serán', 'sería', 'si', 'sí', 'sido', 'siempre', 'siendo', 'siete',
    'sigue', 'siguiente', 'sin', 'sino', 'sobre', 'sois', 'sola', 'solamente',
    'solas', 'solo', 'sólo', 'solos', 'somos', 'son', 'soy', 'su', 'sus',
    'tal', 'también', 'tampoco', 'tan', 'tanto', 'tendrá', 'tendrán', 'teneis',
    'tenemos', 'tener', 'tenga', 'tengo', 'tenía', 'tenido', 'tercera',
    'tiempo', 'tiene', 'tienen', 'toda', 'todas', 'todavía', 'todo', 'todos',
    'total', 'trabaja', 'trabajais', 'trabajamos', 'trabajan', 'trabajar',
    'trabajas', 'trabajo', 'tras', 'trata', 'través', 'tres', 'tuvo', 'tuyo',
    'tu', 'te', 'pq', 'mas', 'qie', 'us', 'has', 'ti', 'ahi', 'mis', 'tus',
    'do', 'X', 'Ven', 'mo', 'Don', 'dia', 'PT', 'sua', 'q', 'x', 'i', 
    'última', 'últimas', 'ultimo', 'último', 'últimos', 'un', 'una', 'unas',
    'uno', 'unos', 'usa', 'usais', 'usamos', 'usan', 'usar', 'usas', 'uso',
    'usted', 'va', 'vais', 'valor', 'vamos', 'van', 'varias', 'varios', 'vaya',
    'veces', 'ver', 'verdad', 'verdadera', 'verdadero', 'vez', 'vosotras',
    'n', 's', 'of', 'c', 'the', 'm', 'qu', 'to', 'as', 'is',
    'asi', 'via', 'sera', 'tambien', 'vosotros', 'voy', 'y', 'ya', 'yo']

def text_to_wordlist(text, remove_stop_words=True, stem_words=False):
    # limpiar el  texto
    text = str(text).strip()
    text = re.sub(r"http\S+", "", text)
    text = re.sub(r"#\S+", "", text)
    #remplazar los acentos
    for s,t in DIACRITICAL_VOWELS:
        text = re.sub(r'{0}'.format(s), t, text)
   #remplazar el SLANG
    for s,t in SLANG:
        text = re.sub(r'\b{0}\b'.format(s), t, text)
    text = re.sub(r"@[A-Za-z0-9]+", ' ', text)
    text = re.sub(r"[^A-Za-z0-9]", " ", text)
    text = re.sub(r"\'s", " ", text)
    text = re.sub(r"\0s", "0", text)
    text = re.sub(r" 9 11 ", "911", text)
    text = re.sub(r"e-mail", "email", text)
    text = re.sub(r"\0rs ", " rs ", text) 
    text = re.sub(r"gps", "GPS", text)
    text = re.sub(r"gst", "GST", text)
   #convertir a minusculas
    text = text.lower()
    # Remove punctuation from text
    text = ''.join([c for c in text if c not in punctuation ])

    # remover stop words
    if remove_stop_words:
        text = text.split()
        text = [w for w in text if not w in stop_words and len(w)>2]
        text = " ".join(text)
    text = re.sub(r' {2,}' , ' ', text)
    return(text.strip())

"""Ejemplo:"""

text_to_wordlist('por Málaga. Pues  #hola  2 ya tengo un motivo más para votar   ')

"""Transformamos las etiquetas a numericas:"""

data['POS'] = data['POS'].apply(lambda x: str(x).strip().upper())
data['POS'].value_counts()

data['label'] = data['POS'].map({"N":0, "NEU":1, "P":2, "NN":0})
data['label'].value_counts()

"""Removemos los duplicados si los hubiera:"""

data.drop_duplicates(inplace=True)
data.shape

"""Removemos valores faltantes, si los hubiera"""

data.fillna(value='', inplace=True)

data.dropna(inplace=True)

"""Dividimos el dataset en train - test"""

from sklearn.model_selection import train_test_split
np.random.seed(4)
data, data_test = train_test_split(data,  test_size=0.2, random_state=42)

data_test.shape

data_test.head()

data.head()

data.shape, data_test.shape

data['full_text'].to_list()[-15:]

data_clean = data['text_changed'].to_list()

data_clean[-15:]

data_labels = data.label.values
data_labels

data_test_clean = data_test['text_changed'].to_list()
data_test_labels = data_test.label.values

"""### Tokenización"""

#tensorflow_text
tokenizer = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(
    data_clean, target_vocab_size=2**16
)

data_inputs = [tokenizer.encode(sentence) for sentence in data_clean]

data_test_inputs = [tokenizer.encode(sentence) for sentence in data_test_clean]

"""### Padding"""

MAX_LEN = max([len(sentence) for sentence in data_inputs])

data_inputs = tf.keras.preprocessing.sequence.pad_sequences(data_inputs,
                                                            value=0,
                                                            padding="post",
                                                            maxlen=MAX_LEN)

data_inputs

data_test_inputs = tf.keras.preprocessing.sequence.pad_sequences(data_test_inputs,
                                                            value=0,
                                                            padding="post",
                                                            maxlen=MAX_LEN)


data_test_inputs

"""### Dividimos en los conjuntos de entrenamiento y de testing

"""

train_inputs = data_inputs
train_labels = data_labels

test_inputs = data_test_inputs
test_labels = data_test_labels

len(train_inputs), len(test_inputs)

"""# Fase 3: Construción del modelo"""

class DCNN(tf.keras.Model):
  # vocal_size Tama;o del vocabulario
  # Dimension de Embeding a 128 numeros de dimension
  # numero de filtros Filtros de 2 3 4 palabras
  # Numero de unidades o numereo de nueronas 512
  # nb_classes =2 en nuestro caso 3  
  # dropout_rate es para evitar el overfiting 
  # training Descativareromos en la fase de entrenamiento
  #self.embedding es la primera capa
  #bigram analizar la palabra de dos en dos y cuantos filtros 
  #kernel size filtrara elementos de dos en dos
  #el padding no es muy importante 
  #activacion 
  #sigmoid es para 2 clases
  #y softmax es para mas de dos clases
  
    def __init__(self,
                 vocab_size,
                 emb_dim=128,
                 nb_filters=50,
                 FFN_units=512,
                 nb_classes=2,
                 dropout_rate=0.1,
                 training=False,
                 name="dcnn"):
        super(DCNN, self).__init__(name=name)
        
        self.embedding = layers.Embedding(vocab_size,
                                          emb_dim ,trainable=True)
        self.bigram = layers.Conv1D(filters=nb_filters,
                                    kernel_size=2,
                                    padding="valid",
                                    activation="relu")
        self.trigram = layers.Conv1D(filters=nb_filters-1,
                                     kernel_size=3,
                                     padding="valid",
                                     activation="relu")
        self.fourgram = layers.Conv1D(filters=nb_filters-2,
                                      kernel_size=4,
                                      padding="valid",
                                      activation="relu")

        self.pool = layers.GlobalMaxPool1D() # No tenemos variable de entrenamiento
                                             # así que podemos usar la misma capa 
                                             # para cada paso de pooling
        self.dense_1 = layers.Dense(units=FFN_units, activation="relu")
        self.dropout = layers.Dropout(rate=dropout_rate)
        if nb_classes == 2:
            self.last_dense = layers.Dense(units=1,
                                           activation="sigmoid")
        else:
            self.last_dense = layers.Dense(units=nb_classes,
                                           activation="softmax")
    
    def call(self, inputs, training):
        x = self.embedding(inputs)
        x_1 = self.bigram(x)
        x_1 = self.pool(x_1)
        x_2 = self.trigram(x)
        x_2 = self.pool(x_2)
        x_3 = self.fourgram(x)
        x_3 = self.pool(x_3)
     
        merged = tf.concat([x_1, x_2, x_3], axis=-1) # (batch_size, 3 * nb_filters)
        merged = self.dense_1(merged)
        merged = self.dropout(merged, training)
        output = self.last_dense(merged)
        
        return output

"""# Paso 4: Aplicación

## Configuración
"""

VOCAB_SIZE = tokenizer.vocab_size # 65540
# dimension 200 numeros en dicho espacio vectorial
EMB_DIM = 200
# es el numero de filtros de la red neuronal convolusional
NB_FILTERS = 400
# es el numero de unidades que el numero de feed forwad tendra en la capa oculta
FFN_UNITS = 128
# el numero de clases en dos clases o dos 
NB_CLASSES = 3#len(set(train_labels))
# la tasa de olvidos es decir que el 20 porciento de las neuroans no se actualizaran
#para evitar el overfiting
DROPOUT_RATE = 0.1
# 
BATCH_SIZE = 16
# vamos a iterar 5 veces 
NB_EPOCHS = 10

"""## Entrenamiento"""

Dcnn = DCNN(vocab_size=VOCAB_SIZE,
            emb_dim=EMB_DIM,
            nb_filters=NB_FILTERS,
            FFN_units=FFN_UNITS,
            nb_classes=NB_CLASSES,
            dropout_rate=DROPOUT_RATE)

if NB_CLASSES == 2:
    Dcnn.compile(loss="binary_crossentropy",
                 optimizer="adam",
                 metrics=["accuracy"])
else:
    Dcnn.compile(loss="sparse_categorical_crossentropy",
                 optimizer="adam",
                 metrics=["sparse_categorical_accuracy"])

#para guardar el chekpoint para guardar la informacion de 
#de check point que nos va a permitir que si se corta la sesión podemos reanudar desde el último check

checkpoint_path = "./model/ckpt/"

ckpt = tf.train.Checkpoint(Dcnn=Dcnn)

ckpt = tf.keras.callbacks.ModelCheckpoint( filepath=checkpoint_path, save_best_only=True, 
                                          monitor='val_sparse_categorical_accuracy',  mode='max', )

# le pasamos bloques de 32 y se corrigen los pesoss
# para evitar problemas de overfiting

Dcnn.fit(data_inputs,
         data_labels,
         batch_size=BATCH_SIZE,
         epochs=NB_EPOCHS,
         validation_split = .2,
         shuffle=False,
         callbacks=[ckpt])

Dcnn.summary()

"""## Evaluación"""

results = Dcnn.evaluate(test_inputs, test_labels, batch_size=BATCH_SIZE)
print(results)

np.argmax(Dcnn.predict(np.array([tokenizer.encode("no me gusta mucho politica")])))

"""## Evaluación en datos de Test"""

from sklearn.metrics import precision_recall_fscore_support
predictions = Dcnn.predict(test_inputs)
all_metrics = precision_recall_fscore_support(test_labels, np.argmax(predictions, axis=1), average='micro')
print("Precision: ", round(all_metrics[0], 2) )
print("Recall: ", round(all_metrics[1], 2) )
print("F1: ", round(all_metrics[2], 2) )

from sklearn.metrics import confusion_matrix

cm= confusion_matrix(test_labels, np.argmax(predictions, axis=1))

def plot_confusion_matrix(cm,
                          target_names,
                          title='Confusion matrix',
                          cmap=None,
                          normalize=True):
 
    import matplotlib.pyplot as plt
    import numpy as np
    import itertools

    accuracy = np.trace(cm) / float(np.sum(cm))
    misclass = 1 - accuracy

    if cmap is None:
        cmap = plt.get_cmap('Blues')

    plt.figure(figsize=(8, 6))
    plt.imshow(cm, interpolation='nearest', cmap=cmap)
    plt.title(title)
    plt.colorbar()

    if target_names is not None:
        tick_marks = np.arange(len(target_names))
        plt.xticks(tick_marks, target_names, rotation=45)
        plt.yticks(tick_marks, target_names)

    if normalize:
        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]


    thresh = cm.max() / 1.5 if normalize else cm.max() / 2
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        if normalize:
            plt.text(j, i, "{:0.4f}".format(cm[i, j]),
                     horizontalalignment="center",
                     color="white" if cm[i, j] > thresh else "black")
        else:
            plt.text(j, i, "{:,}".format(cm[i, j]),
                     horizontalalignment="center",
                     color="white" if cm[i, j] > thresh else "black")


    plt.tight_layout()
    plt.ylabel('True label')
    plt.xlabel('Predicted label\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))
    plt.show()

test_labels.shape

plot_confusion_matrix(cm, target_names = ["N", "NEU", "P"], normalize =False)

from sklearn.metrics import roc_curve,auc, roc_auc_score
import matplotlib.pyplot as plt

roc_auc_score(test_labels, np.array([[y[0], y[1], y[2]] for y in predictions]), multi_class='ovr', labels= [0, 1,2], average="macro")

# Compute ROC curve and ROC area for each class
fpr = dict()
tpr = dict()
roc_auc = dict()
for i in range(3):
    fpr[i], tpr[i], _ = roc_curve(test_labels, np.array([[y[i]] for y in predictions]), pos_label=i)
    roc_auc[i] = auc(fpr[i], tpr[i])

plt.figure(figsize=(8, 6))
lw = 2
colors = (['aqua', 'darkorange', 'cornflowerblue'])
for i, color in zip(range(3), colors):
    plt.plot(fpr[i], tpr[i], color=color, lw=lw,
             label='ROC curve of class {0} (area = {1:0.2f})'
             ''.format(i, roc_auc[i]))


plt.plot([0, 1], [0, 1], 'k--', lw=lw)
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Some extension of Receiver operating characteristic to multi-class')
plt.legend(loc="lower right")

plt.show()

"""## Guardamos el modelo para posterior inferencia"""

Dcnn.save( "dcnn",save_format='tf')

from tensorflow.keras.models import load_model

Dcnn2 = load_model("dcnn")
pred = np.array([tokenizer.encode("no me gusta mucho politica")])
pred = tf.keras.preprocessing.sequence.pad_sequences(pred,
                                                            value=0,
                                                            padding="post",
                                                            maxlen=MAX_LEN)
np.argmax(Dcnn2.predict(pred))

results = Dcnn2.evaluate(test_inputs, test_labels, batch_size=BATCH_SIZE)
print(results)

import pickle

with open('tokenizer.pickle', 'wb') as handle:
    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)

!zip -r  "model_rcnn.zip" "dcnn/"



from google.colab import drive
drive.mount('/content/drive')

!cp  model_rcnn.zip '/content/drive/MyDrive'