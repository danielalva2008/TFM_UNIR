{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analisis del Partido Accion Popular**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tweepy\n",
    "#!pip install pymongo\n",
    "import json\n",
    "import tweepy\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from datetime import datetime as dt\n",
    "import numpy as np\n",
    "from os import path\n",
    "import pymongo\n",
    "from pymongo import MongoClient\n",
    "from pprint import pprint\n",
    "import configparser\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "try:\n",
    "    %tensorflow_version 2.x\n",
    "except Exception:\n",
    "    pass\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import load_model\n",
    "import tensorflow_datasets as tfds\n",
    "import pickle\n",
    "import funciones \n",
    "from funciones import (DIACRITICAL_VOWELS, SLANG, stop_words,SLANG_SP_SN,stop_words_p)\n",
    "from funciones import eliminarhtml,text_to_wordlist,text_to_wordlist_wc,encoding_emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Descargamos los datos de Twitter\n",
    "from tweepy import OAuthHandler\n",
    "#Claves del acceso\n",
    "consumer_key = 'eMcnfrsrAgghHJCCBdP8QStYj'\n",
    "consumer_secret = 'M2FwH7ARB8MK6hq0n8HTrVErWanAsTST0nkmw11VfvuMjn8fel'\n",
    "access_token = '587863980-j6aJNJaXotnCuYF5rBZ8myUYF8kCet61E5sgWWHC'\n",
    "access_token_secret = 'TbSQ6OVufSk0LH6iiRpSAJBwffggEOR2Ah1ToXRQghRIV'\n",
    "\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api=tweepy.API(auth, wait_on_rate_limit=True,wait_on_rate_limit_notify=True,parser=tweepy.parsers.JSONParser())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#guardamos los datos en archivo JSON\n",
    "count = 3000\n",
    "date_since=\"2019-10-01\"\n",
    "search_words=\"#FrenteAmplio\"\n",
    "api = tweepy.API(auth)                                                                                                 \n",
    "results = [status._json for status in tweepy.Cursor(api.search, q=search_words, count=count,since=date_since, tweet_mode='extended', lang='es').items()]\n",
    "results\n",
    "with open('FrenteAmplioS1.json', 'w') as file:\n",
    "    json.dump(results, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "twet_data= open('FrenteAmplioS1.json', 'r').read()\n",
    "twet = json.loads(twet_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1584"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    with open('FrenteAmplioS1.json') as train_file:\n",
    "        dict_train = json.load(train_file, encoding='latin-1')\n",
    "    tweets = pd.DataFrame.from_dict(dict_train)\n",
    "    len(tweets.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Almacenamos los datos mas relevantes\n",
    "question_list = list()\n",
    "for question in tweets.source:\n",
    "  question_list.append(eliminarhtml(str(question).strip()))\n",
    "df = pd.DataFrame(question_list, columns =['source']) \n",
    "df['id_str']         = tweets['id_str']\n",
    "df['created_at']     = tweets['created_at']\n",
    "df['full_text']      = tweets['full_text']\n",
    "df['favorite_count'] = tweets['favorite_count']\n",
    "df['retweet_count']  = tweets['retweet_count']\n",
    "\n",
    "df_3sub=pd.DataFrame(dict([ (k,pd.Series(v)) for k,v in tweets['user'].items() ]))\n",
    "df_3sub.reset_index(col_level=0)\n",
    "df3_cand=df_3sub.T\n",
    "\n",
    "df['screen_name']    = df3_cand['screen_name']\n",
    "df['location']       = df3_cand['location']\n",
    "df['full_text1']     = tweets['full_text']\n",
    "df.drop_duplicates('full_text', keep=\"last\", inplace=True)\n",
    "df.sort_index(inplace=True) \n",
    "\n",
    "#dfx = pd.DataFrame()\n",
    "#dfx['id_str'] =df['id_str']\n",
    "#dfx['full_text_i'] =df['full_text']\n",
    "#dfx = dfx.reset_index(drop=True)\n",
    "#Normalizamos el campo fecha\n",
    "df['created_at'] = pd.to_datetime(df.created_at)\n",
    "df['created_at'] = df['created_at'].dt.normalize()\n",
    "df['numero']=1\n",
    "df['created_at'] = df['created_at'].dt.strftime('%Y-%m-%d')\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_list = list()\n",
    "for question in df.full_text:\n",
    "  question_list.append(text_to_wordlist(str(question).strip()))\n",
    "df1 = pd.DataFrame(question_list, columns =['full_text'])\n",
    "#df1['full_text_i']=dfx['full_text_i']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tokenizador/tokenizer.pickle', 'rb') as handle:\n",
    "    tokenizer = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "487"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Dcnn2 = load_model(\"dcnn\")\n",
    "pred = df1.full_text.apply(tokenizer.encode)\n",
    "pred = tf.keras.preprocessing.sequence.pad_sequences(pred,\n",
    "                                                            value=0,\n",
    "                                                            padding=\"post\",\n",
    "                                                            maxlen=40)\n",
    "len(np.argmax(Dcnn2.predict(pred),axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analisis de Sentimiento\n",
    "my_array=np.argmax(Dcnn2.predict(pred),axis=1)\n",
    "dfp = pd.DataFrame(my_array, columns = ['POS'])\n",
    "dfp['POS1'] = dfp['POS'].map({0:\"Negativo\", 1:\"Neutro\",2:\"Positivo\"})\n",
    "df['POS']=dfp['POS1']\n",
    "df['POS1']=dfp['POS1'].map({\"Negativo\":\"cant_tot_negativos\", \"Neutro\":\"cant_tot_neutro\",\"Positivo\":\"cant_tot_positivos\"})\n",
    "total_frente_amplio = df.groupby(['POS1']).agg(\n",
    "                                  {'numero': 'sum'\n",
    "                                  }).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analisis_sentimiento_frente_amplio = total_frente_amplio.T\n",
    "df_analisis_sentimiento_frente_amplio.reset_index(drop=True,inplace=True)\n",
    "df_analisis_sentimiento_frente_amplio = df_analisis_sentimiento_frente_amplio.rename(columns={0:df_analisis_sentimiento_frente_amplio.loc[0, 0]})\n",
    "df_analisis_sentimiento_frente_amplio = df_analisis_sentimiento_frente_amplio.rename(columns={1:df_analisis_sentimiento_frente_amplio.loc[0, 1]})\n",
    "df_analisis_sentimiento_frente_amplio = df_analisis_sentimiento_frente_amplio.rename(columns={2:df_analisis_sentimiento_frente_amplio.loc[0, 2]})\n",
    "df_analisis_sentimiento_frente_amplio['cant_tot_extraidos']=len(df)\n",
    "df_analisis_sentimiento_frente_amplio.drop([0],axis=0,inplace=True)\n",
    "#df_analisis_sentimiento_frente_amplio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "stop_words = stopwords.words('spanish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_list = list()\n",
    "for question in df.full_text1:\n",
    "  question_list.append(text_to_wordlist_wc(str(question).strip()))\n",
    "df1 = pd.DataFrame(question_list, columns =['full_text1']) \n",
    "#df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_words = '' \n",
    "for val in df1.full_text1: \n",
    "    val = str(val) \n",
    "    # split the value \n",
    "    tokens = val.split() \n",
    "    # Converts each token into lowercase \n",
    "    for i in range(len(tokens)): \n",
    "        tokens[i] = tokens[i].lower() \n",
    "    comment_words += \" \".join(tokens)+\" \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_word=word_tokenize(comment_words)\n",
    "#print(tokenized_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<FreqDist with 2792 samples and 6167 outcomes>\n"
     ]
    }
   ],
   "source": [
    "fdist = FreqDist(tokenized_word)\n",
    "print(fdist)\n",
    "xd=fdist.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analisis de tendencia app\n",
    "df_analisis_tendencia_frente_amplio = pd.DataFrame(data=xd,columns=['termino','frecuencia'])\n",
    "source_df = df.groupby(('source')).numero.sum()\n",
    "source_sorted = sorted(source_df.items(), key=lambda x: x[1], reverse=True)\n",
    "source_t= pd.DataFrame(data=source_sorted,columns=['source','cantidad'])\n",
    "#df_analisis_tendencia_frente_amplio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_t_usuarios_frente_amplio = df.groupby(['created_at','POS']).agg(\n",
    "                                  {'numero': 'sum'\n",
    "                                  }).reset_index()\n",
    "\n",
    "df_t_usuarios_frente_amplio.rename(columns={'created_at': 'fecha', \n",
    "                                    'POS': 'sentimiento','numero':'tweet'}, inplace=True)\n",
    "#df_t_usuarios_frente_amplio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conectamos a la base de datos\n",
    "mongod_connect = 'mongodb://localhost:27017'\n",
    "client = MongoClient(mongod_connect)\n",
    "db =client['dbsistema']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pymongo.results.InsertManyResult at 0x10cd2a64c0>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#creamos la tabla analisis_sentimiento_frente_amplio\n",
    "analisis_sentimiento_frente_amplio = db.analisis_sentimiento_frente_amplio \n",
    "records = json.loads(df_analisis_sentimiento_frente_amplio.T.to_json()).values()\n",
    "db.analisis_sentimiento_frente_amplio.insert_many(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_t_frente_amplio =pd.DataFrame()\n",
    "df_t_frente_amplio['id_str']         =df['id_str']\n",
    "df_t_frente_amplio['screen_name']    =df['screen_name']\n",
    "df_t_frente_amplio['created_at']     =df['created_at']\n",
    "df_t_frente_amplio['full_text']      =df['full_text']\n",
    "df_t_frente_amplio['favorite_count'] =df['favorite_count']\n",
    "df_t_frente_amplio['retweet_count']  =df['retweet_count']\n",
    "df_t_frente_amplio['location']       =df['location']\n",
    "df_t_frente_amplio['source']         =df['source']\n",
    "df_t_frente_amplio['pos']            =df['POS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pymongo.results.InsertManyResult at 0x10ccbaff80>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_usuario_frente_amplio = db.t_usuario_frente_amplio\n",
    "records = json.loads(df_t_frente_amplio.T.to_json()).values()\n",
    "db.t_frente_amplio.insert_many(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pymongo.results.InsertManyResult at 0x10cd2a8100>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analisis_tendencia_frente_amplio = db.analisis_tendencia_frente_amplio\n",
    "records = json.loads(df_analisis_tendencia_frente_amplio.T.to_json()).values()\n",
    "db.analisis_tendencia_frente_amplio.insert_many(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pymongo.results.InsertManyResult at 0x10ce950400>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_usuarios_frente_amplio = db.t_usuarios_frente_amplio\n",
    "records = json.loads(df_t_usuarios_frente_amplio.T.to_json()).values()\n",
    "db.t_usuarios_frente_amplio.insert_many(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
